{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning with Vox Politics\n",
    "\n",
    "For this capstone I have found a data set that contains many articles from Vox. You can see the data set for yourself here:\n",
    "\n",
    "[Vox Articles on data.world](https://data.world/elenadata/vox-articles)\n",
    "\n",
    "Using the main body text of these articles, I would like to create a model that successfully predicts the authors. For this to be more challenging, I will also only use articles that have been categorized as \"Politics & Policy\" so that the contents are generally similar. So let's begin by importing all the necessary modules and taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Modules for making the model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data cleaning and feature importance\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# K-Means module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Spectral Clustering module\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Mean Shift modules\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Metrics to evaluate models\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Support Vector Classification\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>slug</th>\n",
       "      <th>blurb</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin is down 60 percent this year. Here's w...</td>\n",
       "      <td>Timothy B. Lee</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>2014-03-31 14:01:30</td>\n",
       "      <td>2014-12-16 16:37:36</td>\n",
       "      <td>http://www.vox.com/2014/3/31/5557170/bitcoin-b...</td>\n",
       "      <td>Bitcoins have lost more than 60 percent of the...</td>\n",
       "      <td>&lt;p&gt;The markets haven't been kind to&lt;span&gt; &lt;/sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 health problems marijuana could treat better...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>War on Drugs</td>\n",
       "      <td>2014-03-31 15:44:21</td>\n",
       "      <td>2014-11-17 00:20:33</td>\n",
       "      <td>http://www.vox.com/2014/3/31/5557700/six-probl...</td>\n",
       "      <td>Medical marijuana could fill gaps that current...</td>\n",
       "      <td>&lt;p&gt;Twenty states have so far legalized the med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9 charts that explain the history of global we...</td>\n",
       "      <td>Matthew Yglesias</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>2014-04-10 13:30:01</td>\n",
       "      <td>2014-12-16 15:47:02</td>\n",
       "      <td>http://www.vox.com/2014/4/10/5561608/9-charts-...</td>\n",
       "      <td>These nine charts from Thomas Piketty's new bo...</td>\n",
       "      <td>&lt;p&gt;Thomas Piketty's book &lt;i&gt;Capital in the 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remember when legal marijuana was going to sen...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>Criminal Justice</td>\n",
       "      <td>2014-04-03 23:25:55</td>\n",
       "      <td>2014-05-06 21:58:42</td>\n",
       "      <td>http://www.vox.com/2014/4/3/5563134/marijuana-...</td>\n",
       "      <td>Three months after legalizing marijuana, Denve...</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;When Colorado legalized recreational ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obamacare succeeded for one simple reason: it'...</td>\n",
       "      <td>Sarah Kliff</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>2014-04-01 20:26:14</td>\n",
       "      <td>2014-11-18 15:09:14</td>\n",
       "      <td>http://www.vox.com/2014/4/1/5570780/the-two-re...</td>\n",
       "      <td>After a catastrophic launch, Obamacare still s...</td>\n",
       "      <td>&lt;p&gt;There's a very simple reason that Obamacare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title            author  \\\n",
       "0  Bitcoin is down 60 percent this year. Here's w...    Timothy B. Lee   \n",
       "1  6 health problems marijuana could treat better...      German Lopez   \n",
       "2  9 charts that explain the history of global we...  Matthew Yglesias   \n",
       "3  Remember when legal marijuana was going to sen...      German Lopez   \n",
       "4  Obamacare succeeded for one simple reason: it'...       Sarah Kliff   \n",
       "\n",
       "             category       published_date           updated_on  \\\n",
       "0  Business & Finance  2014-03-31 14:01:30  2014-12-16 16:37:36   \n",
       "1        War on Drugs  2014-03-31 15:44:21  2014-11-17 00:20:33   \n",
       "2  Business & Finance  2014-04-10 13:30:01  2014-12-16 15:47:02   \n",
       "3    Criminal Justice  2014-04-03 23:25:55  2014-05-06 21:58:42   \n",
       "4         Health Care  2014-04-01 20:26:14  2014-11-18 15:09:14   \n",
       "\n",
       "                                                slug  \\\n",
       "0  http://www.vox.com/2014/3/31/5557170/bitcoin-b...   \n",
       "1  http://www.vox.com/2014/3/31/5557700/six-probl...   \n",
       "2  http://www.vox.com/2014/4/10/5561608/9-charts-...   \n",
       "3  http://www.vox.com/2014/4/3/5563134/marijuana-...   \n",
       "4  http://www.vox.com/2014/4/1/5570780/the-two-re...   \n",
       "\n",
       "                                               blurb  \\\n",
       "0  Bitcoins have lost more than 60 percent of the...   \n",
       "1  Medical marijuana could fill gaps that current...   \n",
       "2  These nine charts from Thomas Piketty's new bo...   \n",
       "3  Three months after legalizing marijuana, Denve...   \n",
       "4  After a catastrophic launch, Obamacare still s...   \n",
       "\n",
       "                                                body  \n",
       "0  <p>The markets haven't been kind to<span> </sp...  \n",
       "1  <p>Twenty states have so far legalized the med...  \n",
       "2  <p>Thomas Piketty's book <i>Capital in the 21s...  \n",
       "3  <p><span>When Colorado legalized recreational ...  \n",
       "4  <p>There's a very simple reason that Obamacare...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the full TSV file\n",
    "raw_df = pd.read_csv('dsjVoxArticles.tsv', sep='\\t', header=0)\n",
    "\n",
    "# Take a look at it\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded, let's find and take out the top 10 authors in the Politics & Policy category to build our models off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 P&P authors and how many articles they've written:\n",
      "Matthew Yglesias    198\n",
      "Andrew Prokop       174\n",
      "German Lopez        168\n",
      "Sarah Kliff         142\n",
      "Dara Lind           103\n",
      "Dylan Matthews      100\n",
      "Libby Nelson         96\n",
      "Timothy B. Lee       73\n",
      "Ezra Klein           70\n",
      "Jonathan Allen       61\n",
      "Name: author, dtype: int64\n",
      "\n",
      " Total number of articles by the top 10: 1185\n"
     ]
    }
   ],
   "source": [
    "# Take just the politics and policy articles\n",
    "main_df = raw_df[raw_df['category'] == 'Politics & Policy']\n",
    "\n",
    "# Print out the top ten authors from that list and our total # of articles\n",
    "top10 = main_df.author.value_counts()[:10]\n",
    "print(\"Top 10 P&P authors and how many articles they've written:\")\n",
    "print(top10)\n",
    "print('\\n', \"Total number of articles by the top 10:\", top10.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a dataframe with just these authors, and the main body of their articles. We'll also have to clean the text of all the html code that was picked up when the creator of this data set scraped it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Matthew Yglesias</td>\n",
       "      <td>Patricia Arquette's speech about the gender pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Andrew Prokop</td>\n",
       "      <td>Who really matters in our democracy — the gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Andrew Prokop</td>\n",
       "      <td>We've written about gerrymandering here on Vox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Ezra Klein</td>\n",
       "      <td>Presidents consistently overpromise and underd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Dylan Matthews</td>\n",
       "      <td>Let's imagine Daniel and Henry are vacationing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               body\n",
       "21   Matthew Yglesias  Patricia Arquette's speech about the gender pa...\n",
       "53      Andrew Prokop  Who really matters in our democracy — the gene...\n",
       "141     Andrew Prokop  We've written about gerrymandering here on Vox...\n",
       "193        Ezra Klein  Presidents consistently overpromise and underd...\n",
       "209    Dylan Matthews  Let's imagine Daniel and Henry are vacationing..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list from the names of the top 10 and take their works for a new dataframe\n",
    "t10_list = top10.keys()\n",
    "\n",
    "top10_df = main_df[main_df['author'].isin(t10_list)].copy()\n",
    "\n",
    "# Take all of the html coding out of the \"body\" column using beautiful soup\n",
    "top10_df['body'] = [BeautifulSoup(body).get_text() for body in top10_df['body']]\n",
    "\n",
    "# Drop the columns I won't need\n",
    "top10_df = top10_df[['author', 'body']]\n",
    "\n",
    "# Take a look at it\n",
    "top10_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our first two models require uniform sizes of clusters, we will just take the first 61 articles from each of these authors. This still leaves us with 610 total articles, so that should serve our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_unif_df = pd.DataFrame()\n",
    "\n",
    "for author in t10_list:\n",
    "    top10_unif_df = top10_unif_df.append(top10_df[top10_df['author'] == author].iloc[0:61,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Now let's make sure we aren't missing any fields before we dive into the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 610 entries, 21 to 10650\n",
      "Data columns (total 2 columns):\n",
      "author    610 non-null object\n",
      "body      610 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Make sure we aren't missing any fields\n",
    "top10_unif_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're certain our data is ready to use, we can create our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create our feature variable\n",
    "X = top10_unif_df.body\n",
    "\n",
    "# Create our target categories\n",
    "Y = top10_unif_df.author\n",
    "\n",
    "# Split our data up into 25/75 for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Model\n",
    "\n",
    "The data is ready to be modeled, so we'll begin with a K-Means model. Since we know there are 10 authors, this is a logical model to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the distribution of parameters that we want to test\n",
    "kmeans_prm_dist = dict(\n",
    "    kbest__score_func = [f_classif, chi2],\n",
    "    kbest__k = [i for i in np.arange(100, 1_000, 100)],\n",
    "    clf__n_init = [i for i in np.arange(10, 100, 10)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the CV process\n",
    "kmeans_base_pipe = Pipeline([\n",
    "    # Use TfidfVectorizer to select words and get rid of useless stop words\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from\n",
    "    ('kbest', SelectKBest()),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', KMeans(n_clusters = 10))\n",
    "])\n",
    "\n",
    "# Fit the data\n",
    "kmeans_base_model = kmeans_base_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('vect',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2),\n",
       "                                                              norm='l2',\n",
       "                                                              preprocessor=None,\n",
       "                                                              smooth_idf=True,\n",
       "                                                              sto...\n",
       "                                      verbose=False),\n",
       "                   iid=False, n_iter=20, n_jobs=None,\n",
       "                   param_distributions={'clf__n_init': [10, 20, 30, 40, 50, 60,\n",
       "                                                        70, 80, 90],\n",
       "                                        'kbest__k': [100, 200, 300, 400, 500,\n",
       "                                                     600, 700, 800, 900],\n",
       "                                        'kbest__score_func': [<function f_classif at 0x0000029FCB5DC268>,\n",
       "                                                              <function chi2 at 0x0000029FCB5DC378>]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a timer for the SVC's CV search\n",
    "kmcv_start_time = time.time()\n",
    "\n",
    "# Set up the random search with our parameter distribution\n",
    "km_rand_search = RandomizedSearchCV(kmeans_base_model, kmeans_prm_dist, n_iter=20, cv=10, iid=False, verbose=0)\n",
    "\n",
    "# Fit the random search to our data\n",
    "km_rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kbest__score_func': <function f_classif at 0x0000029FCB5DC268>, 'kbest__k': 100, 'clf__n_init': 80}\n",
      "The K-Means' CV search took 1348.9613962173462 seconds\n"
     ]
    }
   ],
   "source": [
    "print(km_rand_search.best_params_)\n",
    "print(\"The K-Means' CV search took \" + str(time.time() - kmcv_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our best parameters to variables to use in an updated pipeline\n",
    "km_score_func = km_rand_search.best_params_['kbest__score_func']\n",
    "km_k = km_rand_search.best_params_['kbest__k']\n",
    "\n",
    "km_n_init = km_rand_search.best_params_['clf__n_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer so that we can see how long it takes\n",
    "km_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the modeling process\n",
    "kmeans_pipe = Pipeline([\n",
    "    # Use TfidfVectorizer to select words and get rid of useless stop words\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from\n",
    "    ('kbest', SelectKBest(score_func = km_score_func, k = km_k)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', KMeans(n_clusters=10, n_init = km_n_init))\n",
    "])\n",
    "\n",
    "# Fit the data\n",
    "kmeans_model = kmeans_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index score: 0.1261550771367049\n",
      "The K-Means model took 6.260534286499023 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "kmeans_pred = kmeans_model.fit_predict(X_test, y_test)\n",
    "\n",
    "# See how well it did\n",
    "print(\"Adjusted Rand Index score:\", metrics.adjusted_rand_score(y_test, kmeans_pred))\n",
    "print(\"The K-Means model took\", time.time() - km_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Adjusted Rand Index score is so close to zero, it looks like this model is close to predicting at random. That's no good! Let's try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering Model\n",
    "\n",
    "As we can again provide the number of clusters to spectral clustering, let's see how this algorithm performs instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the distribution of parameters that we want to test\n",
    "sc_prm_dist = dict(\n",
    "    kbest__score_func = [f_classif, chi2],\n",
    "    kbest__k = [i for i in np.arange(100, 1_000, 100)],\n",
    "    clf__eigen_solver = [None, 'arpack', 'lobpcg'],\n",
    "    clf__n_init = [i for i in np.arange(10, 100, 10)],\n",
    "    clf__assign_labels = ['kmeans', 'discretize']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the CV process\n",
    "sc_base_pipe = Pipeline([\n",
    "    # Use TfidfVectorizer to select words and get rid of useless stop words\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from\n",
    "    ('kbest', SelectKBest()),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', SpectralClustering(n_clusters=10))\n",
    "])\n",
    "\n",
    "# Fit the data\n",
    "sc_base_model = sc_base_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(memory=None,\n         steps=[('vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words='english', strip_accents=None,\n                                 sublinear_tf=True,\n                                 token_patter...\n                                 tokenizer=None, use_idf=True,\n                                 vocabulary=None)),\n                ('kbest',\n                 SelectKBest(k=10,\n                             score_func=<function f_classif at 0x0000029FCB5DC268>)),\n                ('clf',\n                 SpectralClustering(affinity='rbf', assign_labels='kmeans',\n                                    coef0=1, degree=3, eigen_solver=None,\n                                    eigen_tol=0.0, gamma=1.0,\n                                    kernel_params=None, n_clusters=10,\n                                    n_init=10, n_jobs=None, n_neighbors=10,\n                                    random_state=None))],\n         verbose=False) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-175cf8cd3b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Fit the random search to our data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msc_rand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\garaya\\python\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[1;32m--> 608\u001b[1;33m             self.estimator, scoring=self.scoring)\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultimetric_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\garaya\\python\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[1;34m(estimator, scoring)\u001b[0m\n\u001b[0;32m    340\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[0;32m    341\u001b[0m                                                           str):\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mscorers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\garaya\\python\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[1;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 % estimator)\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[1;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(memory=None,\n         steps=[('vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words='english', strip_accents=None,\n                                 sublinear_tf=True,\n                                 token_patter...\n                                 tokenizer=None, use_idf=True,\n                                 vocabulary=None)),\n                ('kbest',\n                 SelectKBest(k=10,\n                             score_func=<function f_classif at 0x0000029FCB5DC268>)),\n                ('clf',\n                 SpectralClustering(affinity='rbf', assign_labels='kmeans',\n                                    coef0=1, degree=3, eigen_solver=None,\n                                    eigen_tol=0.0, gamma=1.0,\n                                    kernel_params=None, n_clusters=10,\n                                    n_init=10, n_jobs=None, n_neighbors=10,\n                                    random_state=None))],\n         verbose=False) does not."
     ]
    }
   ],
   "source": [
    "# Start a timer for the SVC's CV search\n",
    "sccv_start_time = time.time()\n",
    "\n",
    "# Set up the random search with our parameter distribution\n",
    "sc_rand_search = RandomizedSearchCV(sc_base_model, sc_prm_dist, n_iter=20, cv=10, iid=False)\n",
    "\n",
    "# Fit the random search to our data\n",
    "sc_rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc_rand_search.best_params_)\n",
    "print(\"The Spectral Clustering's CV search took \" + str(time.time() - sccv_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our best parameters to variables to use in an updated pipeline\n",
    "sc_score_func = sc_rand_search.best_params_['kbest__score_func']\n",
    "sc_k = sc_rand_search.best_params_['kbest__k']\n",
    "\n",
    "sc_eigen = sc_rand_search.best_params_['clf__eigen_solver']\n",
    "sc_n_init = sc_rand_search.best_params_['clf__n_init']\n",
    "sc_assign = sc_rand_search.best_params_['clf__assign_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer so that we can see how long it takes\n",
    "sc_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the modeling process\n",
    "sc_pipe = Pipeline([\n",
    "    # Use TfidfVectorizer to select words and get rid of useless stop words\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from\n",
    "    ('kbest', SelectKBest(score_func = sc_score_func, k = sc_k)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', SpectralClustering(n_clusters=10, eigen_solver = sc_eigen, n_init = sc_n_init, assign_labels = sc_assign))\n",
    "])\n",
    "\n",
    "# Fit the data\n",
    "sc_model = sc_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "sc_pred = sc_model.fit_predict(X_test, y_test)\n",
    "\n",
    "# See how well it did\n",
    "print(\"Adjusted Rand Index score:\", metrics.adjusted_rand_score(y_test, sc_pred))\n",
    "print(\"The Spectral Clustering model took\", time.time() - sc_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's a little better, but not by much. Let's try another approach then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Shift Model\n",
    "\n",
    "This should work better for our data since the clusters aren't necessarily the same size. Since we don't need uniform sizes for the clusters, we can go back to the full data with > 1000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our feature variable\n",
    "X = top10_df.body\n",
    "\n",
    "# Create our target categories\n",
    "Y = top10_df.author\n",
    "\n",
    "# Split our data up into 25/75 for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the distribution of parameters that we want to test\n",
    "ms_prm_dist = dict(\n",
    "    kbest__score_func = [f_classif, chi2],\n",
    "    kbest__k = [i for i in np.arange(100, 1_000, 100)],\n",
    "    clf__bin_seeding = [True, False],\n",
    "    clf__cluster_all = [True, False]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the CV process\n",
    "ms_base_pipe = Pipeline([\n",
    "    # Use TfidfVectorizer to select words and get rid of useless stop words\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from\n",
    "    ('kbest', SelectKBest()),\n",
    "    \n",
    "    # Transform the sparse matrix into dense data so that MeanShift can use it\n",
    "    ('ftrans', FunctionTransformer(lambda x: x.todense(), validate=False)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', MeanShift())\n",
    "])\n",
    "\n",
    "# Fit the data\n",
    "ms_base_model = ms_base_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start a timer for the SVC's CV search\n",
    "mscv_start_time = time.time()\n",
    "\n",
    "# Set up the random search with our parameter distribution\n",
    "ms_rand_search = RandomizedSearchCV(ms_base_model, ms_prm_dist, n_iter=20, cv=10, iid=False, scoring = 'adjusted_rand_score')\n",
    "\n",
    "# Fit the random search to our data\n",
    "ms_rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ms_rand_search.best_params_)\n",
    "print(\"The Mean Shift's CV search took \" + str(time.time() - mscv_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our best parameters to variables to use in an updated pipeline\n",
    "ms_score_func = ms_rand_search.best_params_['kbest__score_func']\n",
    "ms_k = ms_rand_search.best_params_['kbest__k']\n",
    "\n",
    "ms_bin = ms_rand_search.best_params_['clf__bin_seeding']\n",
    "ms_cluster = ms_rand_search.best_params_['clf__cluster_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer so that we can see how long it takes\n",
    "ms_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the modeling process\n",
    "ms_pipe = Pipeline([\n",
    "    # Use TfidfVectorizer to select words and get rid of useless stop words\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from\n",
    "    ('kbest', SelectKBest(score_func = ms_score_func, k = ms_k)),\n",
    "    \n",
    "    # Transform the sparse matrix into dense data so that MeanShift can use it\n",
    "    ('ftrans', FunctionTransformer(lambda x: x.todense(), validate=False)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', MeanShift(bin_seeding = ms_bin, cluster_all = ms_cluster))\n",
    "])\n",
    "\n",
    "# Fit the data\n",
    "ms_model = ms_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "ms_pred = ms_model.fit_predict(X_test, y_test)\n",
    "\n",
    "# See how well it did\n",
    "print(\"Adjusted Rand Index score:\", metrics.adjusted_rand_score(y_test, ms_pred))\n",
    "print(\"The Spectral Clustering model took\", time.time() - ms_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our feature variable\n",
    "X = top10_df.body\n",
    "\n",
    "# Create our target categories\n",
    "Y = top10_df.author\n",
    "\n",
    "# Split up our training and testing data, 25/75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the distribution of parameters that we want to test\n",
    "svc_prm_dist = dict(\n",
    "    chi__k = [i for i in np.arange(100, 1_000, 100)],\n",
    "    clf__loss = ['hinge', 'squared_hinge'],\n",
    "    clf__C = [i for i in np.arange(0.1,1.1,0.1)],\n",
    "    clf__fit_intercept = [True, False],\n",
    "    clf__max_iter = [i for i in np.arange(5_000, 7_100, 100)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base pipeline that will take us through the modeling process\n",
    "svc_base_pipeline = Pipeline([\n",
    "    # Use TfidfVectorizer and ngram_range = (1,2) to choose 1 and/or 2 words at a time\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from using the chi squared algorithm\n",
    "    ('chi', SelectKBest(chi2)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', LinearSVC(penalty = 'l2')) #'l2' is the standard in SVC\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the SVC's CV search\n",
    "svccv_start_time = time.time()\n",
    "\n",
    "# Set up the random search with our parameter distribution\n",
    "svc_rand_search = RandomizedSearchCV(svc_base_pipeline, svc_prm_dist, n_iter=20, cv=10, iid=False, verbose=0)\n",
    "\n",
    "# Fit the random search to our data\n",
    "svc_rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svc_rand_search.best_params_)\n",
    "print(\"The SVC's CV search took \" + str(time.time() - svccv_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our best parameters to variables to use in an updated pipeline\n",
    "svc_k = svc_rand_search.best_params_['chi__k']\n",
    "\n",
    "svc_loss = svc_rand_search.best_params_['clf__loss']\n",
    "svc_C = svc_rand_search.best_params_['clf__C']\n",
    "svc_fit_intercept = svc_rand_search.best_params_['clf__fit_intercept']\n",
    "svc_max_iter = svc_rand_search.best_params_['clf__max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the modeling process\n",
    "svc_pipeline = Pipeline([\n",
    "    # Use TfidfVectorizer and ngram_range = (1,2) to choose 1 and/or 2 words at a time\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from using the chi squared algorithm\n",
    "    ('chi', SelectKBest(chi2, k = svc_k)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', LinearSVC(\n",
    "                    C = svc_C, \n",
    "                    penalty = 'l2', #'l2' is the standard in SVC \n",
    "                    max_iter = svc_max_iter,\n",
    "                    fit_intercept = svc_fit_intercept,\n",
    "                    loss = svc_loss\n",
    "                    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the SVC model\n",
    "svc_start_time = time.time()\n",
    "\n",
    "# Now we can fit our model to use for predictions\n",
    "svc_model = svc_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See how our model performs\n",
    "print(\"The Support Vector Classification model's accuracy on the test set is: \" + str(svc_model.score(X_test, y_test)))\n",
    "print(\"The Support Vector Classification model took \" + str(time.time() - svc_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svc_true = top10_df.author\n",
    "y_svc_pred = svc_model.predict(top10_df.body)\n",
    "\n",
    "print(classification_report(y_svc_true, y_svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
