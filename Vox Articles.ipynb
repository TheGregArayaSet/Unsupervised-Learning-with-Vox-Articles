{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning with Vox Politics\n",
    "\n",
    "For this capstone I am going to cluster authors just by looking at the articles they have written. For this task I found a data set that contains many articles from the news website Vox. You can see the data set for yourself here: [Vox Articles on data.world](https://data.world/elenadata/vox-articles). Using only the main body text of these articles, I would like to create a model that tries to predict the authors of each article. For this to be more challenging, I will also only use articles that have been categorized as \"Politics & Policy\" so that the contents are generally similar. Through this project I will show my ability to use unsupervised and supervised algorithms, while also unpackaging what their outputs can tell us. Through this I will explore the benefits and challenges you encounter with each method.\n",
    "\n",
    "Since I have already proven my abilities with supervised learning [in my supervised learning capstone where I perform sentiment analysis on Amazon reviews](https://github.com/TheGregArayaSet/Sentiment-Analysis-on-Amazon-Reviews), I would like to focus on unsupervised learning methods to create groups first. I'll start off using the common K-Means algorithm, followed by Spectral Clustering, and then Mean-Shift. Afterwards, I will also run a Support Vector Classification model to compare against. Since we are aware ahead of time how many authors we are trying to predict, the SVC will probably perform somewhat better in terms of predicting the authors outright, but I'm hoping that we can also show an interesting relationship between authors through the combination of all of these models' output information. So, let's begin by importing all the necessary modules and taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Modules for making the model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data cleaning and feature importance\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# K-Means module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Spectral Clustering module\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Mean Shift modules\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Metrics to evaluate models\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Support Vector Classification\n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the modules we'll be using, lets take a look at the data from Vox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>slug</th>\n",
       "      <th>blurb</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin is down 60 percent this year. Here's w...</td>\n",
       "      <td>Timothy B. Lee</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>2014-03-31 14:01:30</td>\n",
       "      <td>2014-12-16 16:37:36</td>\n",
       "      <td>http://www.vox.com/2014/3/31/5557170/bitcoin-b...</td>\n",
       "      <td>Bitcoins have lost more than 60 percent of the...</td>\n",
       "      <td>&lt;p&gt;The markets haven't been kind to&lt;span&gt; &lt;/sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 health problems marijuana could treat better...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>War on Drugs</td>\n",
       "      <td>2014-03-31 15:44:21</td>\n",
       "      <td>2014-11-17 00:20:33</td>\n",
       "      <td>http://www.vox.com/2014/3/31/5557700/six-probl...</td>\n",
       "      <td>Medical marijuana could fill gaps that current...</td>\n",
       "      <td>&lt;p&gt;Twenty states have so far legalized the med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9 charts that explain the history of global we...</td>\n",
       "      <td>Matthew Yglesias</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>2014-04-10 13:30:01</td>\n",
       "      <td>2014-12-16 15:47:02</td>\n",
       "      <td>http://www.vox.com/2014/4/10/5561608/9-charts-...</td>\n",
       "      <td>These nine charts from Thomas Piketty's new bo...</td>\n",
       "      <td>&lt;p&gt;Thomas Piketty's book &lt;i&gt;Capital in the 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remember when legal marijuana was going to sen...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>Criminal Justice</td>\n",
       "      <td>2014-04-03 23:25:55</td>\n",
       "      <td>2014-05-06 21:58:42</td>\n",
       "      <td>http://www.vox.com/2014/4/3/5563134/marijuana-...</td>\n",
       "      <td>Three months after legalizing marijuana, Denve...</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;When Colorado legalized recreational ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obamacare succeeded for one simple reason: it'...</td>\n",
       "      <td>Sarah Kliff</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>2014-04-01 20:26:14</td>\n",
       "      <td>2014-11-18 15:09:14</td>\n",
       "      <td>http://www.vox.com/2014/4/1/5570780/the-two-re...</td>\n",
       "      <td>After a catastrophic launch, Obamacare still s...</td>\n",
       "      <td>&lt;p&gt;There's a very simple reason that Obamacare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title            author  \\\n",
       "0  Bitcoin is down 60 percent this year. Here's w...    Timothy B. Lee   \n",
       "1  6 health problems marijuana could treat better...      German Lopez   \n",
       "2  9 charts that explain the history of global we...  Matthew Yglesias   \n",
       "3  Remember when legal marijuana was going to sen...      German Lopez   \n",
       "4  Obamacare succeeded for one simple reason: it'...       Sarah Kliff   \n",
       "\n",
       "             category       published_date           updated_on  \\\n",
       "0  Business & Finance  2014-03-31 14:01:30  2014-12-16 16:37:36   \n",
       "1        War on Drugs  2014-03-31 15:44:21  2014-11-17 00:20:33   \n",
       "2  Business & Finance  2014-04-10 13:30:01  2014-12-16 15:47:02   \n",
       "3    Criminal Justice  2014-04-03 23:25:55  2014-05-06 21:58:42   \n",
       "4         Health Care  2014-04-01 20:26:14  2014-11-18 15:09:14   \n",
       "\n",
       "                                                slug  \\\n",
       "0  http://www.vox.com/2014/3/31/5557170/bitcoin-b...   \n",
       "1  http://www.vox.com/2014/3/31/5557700/six-probl...   \n",
       "2  http://www.vox.com/2014/4/10/5561608/9-charts-...   \n",
       "3  http://www.vox.com/2014/4/3/5563134/marijuana-...   \n",
       "4  http://www.vox.com/2014/4/1/5570780/the-two-re...   \n",
       "\n",
       "                                               blurb  \\\n",
       "0  Bitcoins have lost more than 60 percent of the...   \n",
       "1  Medical marijuana could fill gaps that current...   \n",
       "2  These nine charts from Thomas Piketty's new bo...   \n",
       "3  Three months after legalizing marijuana, Denve...   \n",
       "4  After a catastrophic launch, Obamacare still s...   \n",
       "\n",
       "                                                body  \n",
       "0  <p>The markets haven't been kind to<span> </sp...  \n",
       "1  <p>Twenty states have so far legalized the med...  \n",
       "2  <p>Thomas Piketty's book <i>Capital in the 21s...  \n",
       "3  <p><span>When Colorado legalized recreational ...  \n",
       "4  <p>There's a very simple reason that Obamacare...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the full TSV file\n",
    "raw_df = pd.read_csv('dsjVoxArticles.tsv', sep='\\t', header=0)\n",
    "\n",
    "# Take a look at it\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some good information here. The title, the author, what the category of the article is, as well as a summary and the full body of the text (with some html code throughout it). This will certainly meet my basic requirements. Let's find the top 10 authors in the Politics & Policy category and put them in their own dataframe to build our models off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total number of articles by the top 10: 1185\n"
     ]
    }
   ],
   "source": [
    "# Take just the politics and policy articles\n",
    "main_df = raw_df[raw_df['category'] == 'Politics & Policy']\n",
    "\n",
    "# Take the top ten authors from that list and their article counts\n",
    "top10 = main_df.author.value_counts()[:10]\n",
    "\n",
    "# Plot the top ten on a bar graph\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.bar(top10.index, height=top10.values, align='center', color=['blue', 'red'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Top 10 Politics & Policy Authors\")\n",
    "plt.ylabel('# of Articles Written')\n",
    "plt.show()\n",
    "\n",
    "print('\\n', \"Total number of articles by the top 10:\", top10.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a dataframe with just our top 10 authors, and the main body of their articles. We'll also have to clean the text of all the html code that was picked up when the data was scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Matthew Yglesias</td>\n",
       "      <td>Patricia Arquette's speech about the gender pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Andrew Prokop</td>\n",
       "      <td>Who really matters in our democracy — the gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Andrew Prokop</td>\n",
       "      <td>We've written about gerrymandering here on Vox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Ezra Klein</td>\n",
       "      <td>Presidents consistently overpromise and underd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Dylan Matthews</td>\n",
       "      <td>Let's imagine Daniel and Henry are vacationing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               body\n",
       "21   Matthew Yglesias  Patricia Arquette's speech about the gender pa...\n",
       "53      Andrew Prokop  Who really matters in our democracy — the gene...\n",
       "141     Andrew Prokop  We've written about gerrymandering here on Vox...\n",
       "193        Ezra Klein  Presidents consistently overpromise and underd...\n",
       "209    Dylan Matthews  Let's imagine Daniel and Henry are vacationing..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list from the names of the top 10 and take their works for a new dataframe\n",
    "t10_list = top10.keys()\n",
    "\n",
    "top10_df = main_df[main_df['author'].isin(t10_list)].copy()\n",
    "\n",
    "# Take all of the html coding out of the \"body\" column using beautiful soup\n",
    "top10_df['body'] = [BeautifulSoup(body).get_text() for body in top10_df['body']]\n",
    "\n",
    "# Drop the columns I won't need\n",
    "top10_df = top10_df[['author', 'body']]\n",
    "\n",
    "# Take a look at it\n",
    "top10_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our first two models require uniform sizes of clusters, we will just take the first 61 articles from each of these authors. This still leaves us with 610 total articles, so that should hopefully serve our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by author, take 61 values, drop any that we don't need and reset the index, then copy it so we don't get warnings\n",
    "top10_unif_df = top10_df.groupby(['author']).head(61).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure we aren't missing any fields before we dive into the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 610 entries, 21 to 10650\n",
      "Data columns (total 2 columns):\n",
      "author    610 non-null object\n",
      "body      610 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Make sure we aren't missing any fields\n",
    "top10_unif_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're certain our data is ready to use, we can create our variables. Following that we can start building and testing out our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign our feature column\n",
    "X = top10_unif_df.body\n",
    "\n",
    "# Assign our target column\n",
    "Y = top10_unif_df.author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Model\n",
    "\n",
    "Since we know there are 10 authors, and this model requires an input number of clusters, this is a logical model to start with. It also tends to run quickly and that will be helpful as we test our ideas. For this initial run I tried a variety of combinations for a few of the arguments within the functions. I chose to use an ngram_range of 2 for the TfidfVectorizer so that it would look at combinations of two words along with just the words themselves (for example it would count up the number of times \"not\", \"great\", and \"not great\" are all found). I tried using larger n_gram ranges, but they didn't improve performance very much and greatly increased time to run. I also told this vectorizer to get rid of useless stop words in the English language, such as \"the\", \"for\", \"and\", etc. For the \"n_init\" argument, which tells the K-Means function the number of times the algorithm should be run with different centroid seeds, I tried a range of #s from 10 to 100 and found 60 performed the best. I also gave K-Means 10 clusters in \"n_clusters\" so that we could hopefully find our 10 authors accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer\n",
    "km_start = time.time()\n",
    "\n",
    "# Instantiate our vectorizer, include bi-grams, and exclude useless stop words\n",
    "km_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Extract features from our dataset\n",
    "X_km = km_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split our data up into 25/75 for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_km, Y, test_size=0.25, random_state = 42)\n",
    "\n",
    "# Instantiate our clustering method\n",
    "kmeans = KMeans(n_clusters = 10, n_init = 60)\n",
    "\n",
    "# Fit the data\n",
    "kmeans_model = kmeans.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index score: 0.1290285433918623\n",
      "The K-Means model took 608.6585247516632 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "kmeans_pred = kmeans_model.fit_predict(X_test, y_test)\n",
    "\n",
    "# See how well it did\n",
    "print(\"Adjusted Rand Index score:\", metrics.adjusted_rand_score(y_test, kmeans_pred))\n",
    "print(\"The K-Means model took\", time.time() - km_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Adjusted Rand Index score is closer to zero than it is to 1, it looks like this model is close to predicting at random. That's no good! I wonder what the clusters look like, let's look at a contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Andrew Prokop</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dara Lind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dylan Matthews</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ezra Klein</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German Lopez</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jonathan Allen</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Libby Nelson</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthew Yglesias</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sarah Kliff</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timothy B. Lee</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0             0   1   2   3   4  5  6  7  8   9\n",
       "author                                             \n",
       "Andrew Prokop     0   0   1  10   1  1  4  1  0  18\n",
       "Dara Lind         0   0   1  14   0  5  2  1  0   4\n",
       "Dylan Matthews    2   1   2   2   2  1  1  1  0  11\n",
       "Ezra Klein        0   0   1   1   7  0  0  4  0   3\n",
       "German Lopez      3   0   2   0  14  0  8  0  5   8\n",
       "Jonathan Allen    1   1   1   1   1  0  0  0  0  15\n",
       "Libby Nelson      1  11   1   0   6  0  1  0  0   5\n",
       "Matthew Yglesias  1   0  11   6   7  1  0  3  0  24\n",
       "Sarah Kliff       0   0   4   1  33  0  0  0  0   2\n",
       "Timothy B. Lee    1   0   2   4   3  1  1  1  2   2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predicted clusters\n",
    "pd.crosstab(y_test, kmeans_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly didn't work out as well for us with 10 clusters, but maybe there's a reason for that. I'll try an interesting range of cluster sizes and see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Adjusted Rand Index score using 4 clusters is : 0.06854323843426914\n",
      "The Adjusted Rand Index score using 5 clusters is : 0.12421069780848354\n",
      "The Adjusted Rand Index score using 8 clusters is : 0.13028772900253466\n",
      "The Adjusted Rand Index score using 9 clusters is : 0.12774493723116817\n",
      "The Adjusted Rand Index score using 11 clusters is : 0.14227973627165258\n",
      "The Adjusted Rand Index score using 12 clusters is : 0.1395839474711459\n",
      "The Adjusted Rand Index score using 14 clusters is : 0.1781061874137374\n",
      "The Adjusted Rand Index score using 15 clusters is : 0.20278719000640466\n",
      "The Adjusted Rand Index score using 20 clusters is : 0.24410451086524743\n"
     ]
    }
   ],
   "source": [
    "interesting_range = [4, 5, 8, 9, 11, 12, 14, 15, 20]\n",
    "\n",
    "for nx in interesting_range:\n",
    "    # Instantiate our clustering method\n",
    "    kmeans = KMeans(n_clusters = nx, n_init = 60)\n",
    "\n",
    "    # Fit the data\n",
    "    kmeans_model = kmeans.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    kmeans_pred = kmeans_model.fit_predict(X_test, y_test)\n",
    "\n",
    "    # See how well it did\n",
    "    print(\"The Adjusted Rand Index score using {} clusters is : {}\".format(nx, metrics.adjusted_rand_score(y_test, kmeans_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 15 and 20 clusters did well in comparison. Let's see what their contingency tables look like to get some insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Contingency Table using 15 clusters:\n",
      "col_0             0   1   2   3   4   5   6   7   8   9   10  11  12  13  14\n",
      "author                                                                      \n",
      "Andrew Prokop      2   9   0   3   3   2   0   0   0   0   3   0   0   1   0\n",
      "Dara Lind          0   4   0   1   0   6   0   0   5   0   0   2   0   0   0\n",
      "Dylan Matthews     0   3   0   0   1   0   0   0   0   0   3   0   0   2   0\n",
      "Ezra Klein         0   2   0   3   1   0   0   1   0   2   2   0   0   2   0\n",
      "German Lopez       0   0   4   0   3   0   0   1   1   0   5   0   2   0   1\n",
      "Jonathan Allen     0  11   0   1   0   0   0   0   0   1   6   0   0   0   0\n",
      "Libby Nelson       0   2   0   0   0   2   7   0   2   1   0   0   0   1   0\n",
      "Matthew Yglesias   0   2   0   3   0   0   1   1   0   1   0   0   0   5   0\n",
      "Sarah Kliff        0   0   1   0   1   1   0  10   0   0   2   0   0   1   0\n",
      "Timothy B. Lee     0   2   0   0   0   0   0   0   1   3   1   0   2   0   1 \n",
      "\n",
      "The Contingency Table using 20 clusters:\n",
      "col_0             0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  \\\n",
      "author                                                                         \n",
      "Andrew Prokop      1   0   3   4   1   0   0   4   0   1   0   2   1   0   0   \n",
      "Dara Lind          0  10   0   0   0   0   1   0   0   2   0   2   0   1   0   \n",
      "Dylan Matthews     0   0   1   2   0   0   1   1   0   1   0   0   0   0   2   \n",
      "Ezra Klein         0   0   5   0   0   0   0   0   0   0   3   2   0   0   1   \n",
      "German Lopez       1   0   0   0   1   4   0   3   2   2   0   0   0   2   2   \n",
      "Jonathan Allen     3   0   0   1   0   0   0   0   1   8   1   1   0   0   0   \n",
      "Libby Nelson       1   0   0   0   0   0   9   1   0   0   0   0   0   1   0   \n",
      "Matthew Yglesias   0   0   1   0   0   0   1   0   0   1   0   0   0   0   4   \n",
      "Sarah Kliff        0   0   7   0   3   1   0   0   1   0   0   0   0   0   1   \n",
      "Timothy B. Lee     0   0   0   0   0   1   0   0   0   3   3   0   1   2   0   \n",
      "\n",
      "col_0             15  16  17  18  19  \n",
      "author                                \n",
      "Andrew Prokop      2   2   2   0   0  \n",
      "Dara Lind          0   0   1   0   1  \n",
      "Dylan Matthews     0   0   1   0   0  \n",
      "Ezra Klein         0   0   0   0   2  \n",
      "German Lopez       0   0   0   0   0  \n",
      "Jonathan Allen     0   0   4   0   0  \n",
      "Libby Nelson       0   0   1   1   1  \n",
      "Matthew Yglesias   0   1   0   0   5  \n",
      "Sarah Kliff        0   0   0   3   0  \n",
      "Timothy B. Lee     0   0   0   0   0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nx in [15, 20]:\n",
    "    # Instantiate our clustering method\n",
    "    kmeans = KMeans(n_clusters = nx, n_init = 60)\n",
    "\n",
    "    # Fit the data\n",
    "    kmeans_model = kmeans.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    kmeans_pred = kmeans_model.fit_predict(X_test, y_test)\n",
    "\n",
    "    # See how well it did\n",
    "    print(\"The Contingency Table using {} clusters:\".format(nx))\n",
    "    print(pd.crosstab(y_test, kmeans_pred), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These had varied results, however these groupings could still tell us something. For example, Libby Nelson consistently stands out. In the cluster of 10 she gets 11 articles in one group while there are only 2 other articles in that group (see column 2), with 15 clusters she almost got a whole cluster to herself with 7 articles (see column 6), and in the 20 cluster table she consisted of 9/13 articles in the 6th cluster. I also noticed that Dara Lind and Sarah Kliff were able to conquer a few clusters for themselves in each of our tests. It's possible that some people have pretty distinct styles of writing, while others tend to go with the safe and normal flow. Especially in this time of hyper political correctness, the editing process is probably quite restricting. However, unfortunately none of the chosen cluster sizes did all that much better than above. Maybe we should try and look at another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Model\n",
    "\n",
    "As we can again provide the number of clusters we expect to spectral clustering, let's see how this algorithm performs with 10 groups. I tried multiple ngram_range values for the TfidfVectorizer, like above, but it didn't increase accuracy all that much and greatly increased time to run. As for the Spectral Clustering parameters themselves, I simply tried a range of combinations of the variables that you see and kept the rest at their defaults. I did this by running the 10 clusters over and over and keeping a separate spreadsheet where I recorded values and reran it. This allowed me to learn a bit about how each variable works. I didn't want to include all of the combinations in this report as it would just end up being really long, and uninformative for the experienced reader. This process is most likely less interesting than the actual results anyways, so let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer\n",
    "sc_start = time.time()\n",
    "\n",
    "# Instantiate our vectorizer, include bi-grams, and exclude useless stop words\n",
    "sc_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Extract features from our dataset\n",
    "X_sc = sc_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split our data up into 25/75 for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc, Y, test_size=0.25, random_state = 42)\n",
    "\n",
    "# Instantiate our clustering method\n",
    "spect_clust = SpectralClustering(n_clusters=10, eigen_solver = None, n_init = 10, assign_labels = 'discretize')\n",
    "\n",
    "# Fit the data\n",
    "sc_model = spect_clust.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index score: 0.1782984424287866\n",
      "The Spectral Clustering model took 1.7532403469085693 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "sc_pred = sc_model.fit_predict(X_test, y_test)\n",
    "\n",
    "# See how well it did\n",
    "print(\"Adjusted Rand Index score:\", metrics.adjusted_rand_score(y_test, sc_pred))\n",
    "print(\"The Spectral Clustering model took\", time.time() - sc_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to not have done so well either. Well, let's take a look at how the clusters were predicted in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Andrew Prokop</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dara Lind</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dylan Matthews</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ezra Klein</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German Lopez</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jonathan Allen</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Libby Nelson</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthew Yglesias</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sarah Kliff</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timothy B. Lee</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0              0  1   2  3  4  5   6   7  8  9\n",
       "author                                            \n",
       "Andrew Prokop      8  2   0  2  0  0  10   1  0  0\n",
       "Dara Lind          2  0  15  0  0  0   1   0  0  0\n",
       "Dylan Matthews     2  2   1  0  0  0   3   1  0  0\n",
       "Ezra Klein         3  1   1  0  0  0   3   3  2  0\n",
       "German Lopez       0  3   0  0  0  2   6   1  0  5\n",
       "Jonathan Allen    15  0   0  0  0  0   1   0  1  2\n",
       "Libby Nelson       0  0   0  0  5  0  10   0  0  0\n",
       "Matthew Yglesias   1  3   0  0  0  0   8   1  0  0\n",
       "Sarah Kliff        0  1   0  0  0  0   5  10  0  0\n",
       "Timothy B. Lee     2  0   0  0  0  1   4   0  3  0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predicted clusters\n",
    "pd.crosstab(y_test, sc_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this didn't do so well, it looks like Libby Nelson got a column to herself again (see column 4). Dara Lind and Sarah Kliff also seemed to get clustered quite well too. Let's try a range of clusters again like we did for K-Means, maybe we can find an ideal grouping or two to try that can give us further insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Adjusted Rand Index score using 4 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 5 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 8 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 9 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 11 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 12 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 14 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 15 clusters is : 0.11404203717117059\n",
      "The Adjusted Rand Index score using 20 clusters is : 0.11404203717117059\n"
     ]
    }
   ],
   "source": [
    "for nx in interesting_range:\n",
    "    # Instantiate our clustering method\n",
    "    spect_clust = SpectralClustering(n_clusters=nx, eigen_solver = None, n_init = 10, assign_labels = 'discretize')\n",
    "\n",
    "    # Fit the data\n",
    "    sc_model = spect_clust.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    sc_pred = sc_model.fit_predict(X_test, y_test)\n",
    "\n",
    "    # See how well it did\n",
    "    sent = \"The Adjusted Rand Index score using {} clusters is : {}\"\n",
    "    print(sent.format(nx, metrics.adjusted_rand_score(y_test, kmeans_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well seeing as how these all got the same score for each of these cluster values, we probably won't get much more out of this model, let's try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Shift Model\n",
    "\n",
    "I decided to try a mean-shift model next because in theory it should work for this data, since the clusters aren't necessarily the same size. Since we don't need uniform sizes for the clusters, we can go back to the full data with > 1000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our feature column\n",
    "X = top10_df.body\n",
    "\n",
    "# Assign our target column\n",
    "Y = top10_df.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer\n",
    "ms_start = time.time()\n",
    "\n",
    "# Instantiate our vectorizer, include bi-grams, and exclude useless stop words\n",
    "ms_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Extract features from our dataset\n",
    "X_ms = ms_vectorizer.fit_transform(X)\n",
    "\n",
    "# Transform the sparse matrix of features into dense data so mean shift can use it\n",
    "F = FunctionTransformer(lambda x: x.todense(), validate=False)\n",
    "X_ms = F.transform(X_ms)\n",
    "\n",
    "# Split our data up into 25/75 for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ms, Y, test_size=0.25, random_state = 42)\n",
    "\n",
    "# Instantiate our clustering method\n",
    "mean_shift = MeanShift(bin_seeding = True, cluster_all = False)\n",
    "\n",
    "# Fit the data\n",
    "ms_model = mean_shift.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index score: 0.0\n",
      "The Mean Shift model took 665.3452394008636 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "ms_pred = ms_model.fit_predict(X_test, y_test)\n",
    "\n",
    "# See how well it did\n",
    "print(\"Adjusted Rand Index score:\", metrics.adjusted_rand_score(y_test, ms_pred))\n",
    "print(\"The Mean Shift model took\", time.time() - ms_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as how this model got a 0 Adjusted Rand Index score, there isn't any point in looking at the contingency table. What this tells us is that, most likely we don't have a varied enough data set. Essentially, it put everyone in the same group. It would probably be helpful if we pooled articles from different websites about the same topics, or just got more articles in general. Regardless, it is a telling sign, so I decided to keep this model in this write up even though it does so poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know for sure there should be 10 authors, we can try out a supervised learning model too. In my supervised learning capstone project, I had good recall and precision scores when using Support Vector Classification, so I'm going to try and see if I can guess the authors by their articles' contents. I will also be performing a Cross Validation search and a Pipeline in order to automate the steps that I was doing above, and to get the best parameters we can for this run. Let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our feature column\n",
    "X = top10_df.body\n",
    "\n",
    "# Assign our target column\n",
    "Y = top10_df.author\n",
    "\n",
    "# Split up our training and testing data, 25/75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the distribution of parameters that we want to test\n",
    "svc_prm_dist = dict(\n",
    "    chi__k = [i for i in np.arange(100, 1_000, 100)],\n",
    "    clf__loss = ['hinge', 'squared_hinge'],\n",
    "    clf__C = [i for i in np.arange(0.1,1.1,0.1)],\n",
    "    clf__fit_intercept = [True, False],\n",
    "    clf__max_iter = [i for i in np.arange(8_500, 10_100, 100)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base pipeline that will take us through the modeling process\n",
    "svc_base_pipeline = Pipeline([\n",
    "    # Use TfidfVectorizer and ngram_range = (1,2) to choose 1 and/or 2 words at a time\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from using the chi squared algorithm\n",
    "    ('chi', SelectKBest(chi2)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', LinearSVC(penalty = 'l2')) #'l2' is the standard in SVC\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a distribution of parameters that we want to try out, as well as a pipeline to push our work through, we can begin a cross validation search to find out best parameters. I chose a randomized search because it will be faster than doing a full grid search. NOTE: During this randomized search during some iterations the model is unable to converge and produces warnings, so I'm going to ignore any warnings just for this bit. The cross validation search does eventually converge and achieve what we need however so that's why it prints a warning instead of an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the SVC's CV search\n",
    "svccv_start_time = time.time()\n",
    "\n",
    "# Temporarily blocks warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up the random search with our parameter distribution\n",
    "svc_rand_search = RandomizedSearchCV(svc_base_pipeline, svc_prm_dist, n_iter=20, cv=10, iid=False, verbose=0)\n",
    "\n",
    "# Fit the random search to our data\n",
    "svc_rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_iter': 9200, 'clf__loss': 'squared_hinge', 'clf__fit_intercept': False, 'clf__C': 0.9, 'chi__k': 800}\n",
      "The SVC's CV search took 443.01765418052673 seconds\n"
     ]
    }
   ],
   "source": [
    "print(svc_rand_search.best_params_)\n",
    "print(\"The SVC's CV search took \" + str(time.time() - svccv_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found our optimal parameters and printed them out above, let's plug them into another pipeline and get some descriptive statistics about each author. We will achieve this by looking at a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our best parameters to variables to use in an updated pipeline\n",
    "svc_k = svc_rand_search.best_params_['chi__k']\n",
    "\n",
    "svc_loss = svc_rand_search.best_params_['clf__loss']\n",
    "svc_C = svc_rand_search.best_params_['clf__C']\n",
    "svc_fit_intercept = svc_rand_search.best_params_['clf__fit_intercept']\n",
    "svc_max_iter = svc_rand_search.best_params_['clf__max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that will take us through the modeling process\n",
    "svc_pipeline = Pipeline([\n",
    "    # Use TfidfVectorizer and ngram_range = (1,2) to choose 1 and/or 2 words at a time\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words='english', sublinear_tf=True)),\n",
    "    \n",
    "    # Select the best features to train from using the chi squared algorithm\n",
    "    ('chi', SelectKBest(chi2, k = svc_k)),\n",
    "    \n",
    "    # Choose our classifier algorithm\n",
    "    ('clf', LinearSVC(\n",
    "                    C = svc_C, \n",
    "                    penalty = 'l2', #'l2' is the standard in SVC \n",
    "                    max_iter = svc_max_iter,\n",
    "                    fit_intercept = svc_fit_intercept,\n",
    "                    loss = svc_loss\n",
    "                    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer for the SVC model\n",
    "svc_start_time = time.time()\n",
    "\n",
    "# Now we can fit our model to use for predictions\n",
    "svc_model = svc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Return warnings to normal\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Support Vector Classification model's accuracy on the test set is: 0.5420875420875421\n",
      "The Support Vector Classification model took 2.680087089538574 seconds\n"
     ]
    }
   ],
   "source": [
    "# See how our model performs\n",
    "print(\"The Support Vector Classification model's accuracy on the test set is: \" + str(svc_model.score(X_test, y_test)))\n",
    "print(\"The Support Vector Classification model took \" + str(time.time() - svc_start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   Andrew Prokop       0.48      0.86      0.62       174\n",
      "       Dara Lind       0.75      0.71      0.73       103\n",
      "  Dylan Matthews       0.74      0.20      0.31       100\n",
      "      Ezra Klein       1.00      0.11      0.21        70\n",
      "    German Lopez       0.69      0.87      0.77       168\n",
      "  Jonathan Allen       0.82      0.23      0.36        61\n",
      "    Libby Nelson       0.71      0.85      0.77        96\n",
      "Matthew Yglesias       0.62      0.43      0.51       198\n",
      "     Sarah Kliff       0.67      0.90      0.77       142\n",
      "  Timothy B. Lee       0.74      0.67      0.71        73\n",
      "\n",
      "        accuracy                           0.64      1185\n",
      "       macro avg       0.72      0.58      0.57      1185\n",
      "    weighted avg       0.68      0.64      0.60      1185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_svc_true = top10_df.author\n",
    "y_svc_pred = svc_model.predict(top10_df.body)\n",
    "\n",
    "print(classification_report(y_svc_true, y_svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This supervised model performed better than the unsupervised models specifically at predicting the authors accurately, but that's not entirely surprising. Supervised learning is a better method when you have a specified outcome, while unsupervised learning is better when you don't. For example, if we want to see patterns of behavior, we turn to unsupervised learning. In this example specifically, that's like saying I'm trying to understand different author's writing styles and how they are similar or different from each other's. I think in this way they still gave us some interesting outcomes. There were a few authors who seemed to stick out from the pack in terms of originality (i.e. getting clustered by themselves more often than others) in each of our unsupervised models, such as Libby Nelson, Sarah Kliff, and Dara Lind. Interestingly, our supervised learning model was also better at predicting these authors outright, giving them all f1-scores (i.e. a weighted version of precision and recall) above 70%. This indicates that they most likely do indeed have unique writing styles, at least as opposed to other authors in our data. Kudos to them and their careers.\n",
    "\n",
    "However, most of these authors did get clumped together most of the time, which could be due to multiple factors. The team at Vox mainly reside on one side of the political aisle (Democrats) as can be seen on rating websites when you look them up. Therefore it is likely that they are taking quite similar stances on issues even if different people are writing about the developments of new stories on different days. On top of this, during the editing process, other authors are probably striking out, or at least mitigating, any ideas or phrases that might not fly. This likely serves to make a more uniformed version of each article before it is officially released, thus stripping it of some of its originality. Another issue is that the amount of data being used was relatively small. If I had looked at more categories, or if I could have scraped more articles by each of these authors myself, this could have helped to boost our ending scores. If I were to do this sort of project over again, I would try and include multiple sites, and I would increase the total number of articles quite a bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
